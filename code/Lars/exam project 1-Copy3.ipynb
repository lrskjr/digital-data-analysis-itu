{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "59f5868a",
   "metadata": {},
   "source": [
    "# Exam details\n",
    "Written work:  \n",
    "\n",
    "You will have to analyze a dataset and test a relevant hypothesis.\n",
    "\n",
    "You should choose your own dataset by Lecture 12 (written hand-in already available)\n",
    "\n",
    "The report will be written as a Jupiter notebook.\n",
    "\n",
    "Minimal set of analysis:\n",
    "1) Describe your data and visualize some key dimensions.\n",
    "2) Perform at least two analysis (depending on what is appropriate given the data you selected):\n",
    "- Does your data contain quantitative values that allow for a hypothesis testing?\n",
    "   IF YES: Formulate an hypothesis and test it. Complement the testing with an appropriate visualization.\n",
    "- Does your data contain unstructured textual information?\n",
    " IF YES: Perform sentiment analysis on your data and describe and visualize the results.\n",
    "- Does your data contain network structures (or a network structure can be extracted)?\n",
    " IF YES: Ask a question about the network structure and answer it.\n",
    "\n",
    "OBS: Different datasets can be investigated in many different ways. Any combination of the above-described analysis is acceptable as long as you ask 2 questions. \n",
    "e.g. Statistical hypothesis testing + network analysis, Sentiment analysis + network analysis , Statistical hypothesis testing + Statistical hypothesis testing.\n",
    "\n",
    "Groups:  Find your own group (2 ppls - 3 is possible) let the TAs know before Fall break.\n",
    "\n",
    "Opgaven skal indeholde en eller flere af følgende: \n",
    "- Hypothesis testing\n",
    "- Sentiment Analyse\n",
    "- Network Analysis \n",
    "\n",
    "Desuden også\n",
    "- Interactive Visualization "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06f89f00",
   "metadata": {},
   "source": [
    "# Data\n",
    "Datasættet stammer fra \"The Danish Parliament Corpus 2009 - 2017, v2, w. subject annotation\".\n",
    "\n",
    "Kilde: \n",
    "Hansen, Dorte Haltrup and Navarretta, Costanza, 2021, The Danish Parliament Corpus 2009 - 2017, v2, w. subject annotation, CLARIN-DK-UCPH Centre Repository, http://hdl.handle.net/20.500.12115/44.\n",
    "\n",
    "\n",
    "Datasættet består af transkriptioner af taler i Folketinget fra første samling 2009 til og med først samling 2016 (6/10 2009 – 7/9 2017). Til hver tale er der tilknyttet metadata, dels om medlemmet af folketinget ('Name', 'Gender', 'Party', 'Role', 'Title', 'Birth', 'Age'), dels om talen (Date', 'samling', 'Start time', 'End time', 'Time', 'Agenda item', 'Case no', 'Case type', 'Agenda title', 'Subject 1', 'Subject 2').\n",
    "\n",
    "Datasættet er struktureret i tsv txt-filer, som er formateret i utf-8. Der er en fil per møde.\n",
    "\n",
    "Kilde:\n",
    "Samme, Readme\n",
    "\n",
    "\n",
    "Til denne opgave har vi samlet tsv filerne i et nyt datasæt, som vi har gemt i en csv fil separeret med pipes. Csv filen er uploadet til sciencedata.dk, hvorfra den kan downloades via url med pandas.read_csv() metoden.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Emnet\n",
    "Emnet er immigrationspolitik fra 2009 - 2017. Hvilke kendetegn har de forskellige partiers politik vurdereret ud fra partimedlemers taler i Folketinget?\n",
    "\n",
    "1. Ved hjælp af Tf-Idf identificerer vi de særegne nøgleord, der kendetegner de forskellige partier.\n",
    "\n",
    "2. sentiment analyse på taler som indeholder noget om flygtning. Det kan f.eks. være taler, som handler om 'os' og 'dem' - nærlæsning. Bliver der større variation i sentiment-scorrerne mere varieret op til et valg? Bliver de mere varieret omkring 2015?  \n",
    "3. pos -tag f.eks. verber fra forskellige partier, og hvilke adjektiver knytters sig til et begreb. \n",
    "\n",
    "\n",
    "# Load data\n",
    "\n",
    "Data ligger på sciencedata.dk og deles derfra med download link."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "407c096e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c964c8e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data from sciencedata.dk\n",
    "df = pd.read_csv('https://sciencedata.dk/shared/825e999a5c13fd22d28d4289fa899ba1?download', sep='|')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a1df7ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "print (f'The coloumns are {df.columns}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1243e60d",
   "metadata": {},
   "source": [
    "# Question 1\n",
    "# TF-IDF. Segment = speech \n",
    "\n",
    "1. Ved hjælp af Tf-Idf identificerer vi de særegne nøgleord, der kendetegner de forskellige partier.\n",
    "\n",
    "Method:\n",
    "1. Subset data on the subject value \"Immigration\" and on the role value \"member\". Group on session and party and aggregate the speeches according to the groups.  \n",
    "2. Preprocess the texts using a stopword list and ***Spacy*** to lemmatize words in speeches.\n",
    "3. Use **Tf-Idf** to identify distinctive keywords\n",
    "\n",
    "\n",
    "\n",
    "Der skal renses bedre, fordi spacy lemmatizer løber tør for plads med så mange ord.\n",
    "Jeg kan fjerne ord med tal, tal og ord mindre end to bogstaver.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab779391",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subset data\n",
    "input_data = df[(df['Subject 1'] == 'Immigration') & (df['Role'] == 'medlem')].reset_index()\n",
    "# Group by 'session' and 'party' and aggregate speeches\n",
    "input_data_grouped = df.groupby(['samling', 'Party'])['Text'].agg(' '.join).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fafb6b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess\n",
    "import re\n",
    "import time\n",
    "from urllib.request import urlopen\n",
    "startTime = time.time()\n",
    "\n",
    "def scrub_text(text):\n",
    "    # Remove all numbers (including integers and decimals)\n",
    "    text_without_numbers = re.sub(r'\\b\\d+(\\.\\d+)?\\b', '', text)\n",
    "\n",
    "    # Remove words that include numbers\n",
    "    text_without_words_with_numbers = re.sub(r'\\w*\\d\\w*', '', text_without_numbers)\n",
    "    \n",
    "    # find all none whitspace characters between two word bounderies\n",
    "    return re.findall(r'\\b\\S+\\b', text_without_words_with_numbers.lower().replace('_', ' ')) \n",
    "\n",
    "# load stopword list from sciencedata.dk\n",
    "with urlopen('https://sciencedata.dk/shared/01f12a3094769a0a6c66fdd0d7bb7dac?download') as response:\n",
    "    stop_words = response.read().decode('utf-8-sig').split('\\r\\n')    \n",
    "    \n",
    "    \n",
    "def filter_stopword(text_list):\n",
    "    return [i for i in text_list if i not in stop_words]\n",
    "\n",
    "def remove_short_words(text_list):\n",
    "    return [i for i in text_list if len(i) > 2]\n",
    "  \n",
    "print('speeches')    \n",
    "speeches = input_data_grouped['Text'].tolist()\n",
    "\n",
    "print('clean_strings_in_list')\n",
    "clean_strings_in_list = [scrub_text(i) for i in speeches]\n",
    "\n",
    "print('strings_wo_stop_words')\n",
    "strings_wo_stop_words = [filter_stopword(text) for text in clean_strings_in_list] \n",
    "\n",
    "print('filter_short_words')\n",
    "remove_short_words = [remove_short_words(text) for text in strings_wo_stop_words] \n",
    "\n",
    "print('strings')\n",
    "strings = [' '.join(i) for i in remove_short_words]\n",
    "\n",
    "executionTime = (time.time() - startTime)\n",
    "print('Execution time in seconds: ' + str(executionTime))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bc6ca30",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data_grouped['Clean_text_wo_sw'] = strings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9339b2fc",
   "metadata": {},
   "source": [
    "Hvis jeg kører Spacy-lemmatizer-koden nedenfor får jeg en valueerror:\n",
    "\n",
    "_ValueError: [E088] Text of length 1975499 exceeds maximum of 1000000. The parser and NER models require roughly 1GB of temporary memory per 100,000 characters in the input. This means long texts may cause memory allocation errors. If you're not using the parser or NER, it's probably safe to increase the `nlp.max_length` limit. The limit is in number of characters, so you can check whether your inputs are too long by checking `len(text)`._\n",
    "\n",
    "For at koorigere den plan vi har haft, kunne vi opdele data i mindre subsets, applicere Spacy lemmatizeren på mindre dele og samle datasættet igen. Vi er interesseret i at anvende Spacy frem for NLTK, fordi NLTK har ikke en lematizer, der kan benyttes på dansk tekst. Clarin-dk har udviklet en lemmatizer til dansk tekst, men den er ufleksibel, fordi den skal benyttes via et webinterface og kan ikke håndtere de mængder af data, som har arbejder med.\n",
    "\n",
    "Vores problem er at tekstlængden på 1975499, som giver en valueerror er kun en brøkdel af vores data. Det er kun den del, der består af alle DF medlemmers taler inden for emnet 'immigration' i samlingen 20091.\n",
    "\n",
    "Vi kan observere, at det er krævende for en computer at arbejde med Spacy, når det kommer til store tekstmængder, hvilket er problematisk, fordi flere og flere studerende og professionelle arbejder med text mining på almindelige, gennemsnitlige computere.\n",
    "\n",
    "Problemet bliver behandlet på online fællesskaber som Stack overflow og Stackexchange, for eksempel i denne blogpost \n",
    "_Increasing SpaCy max NLP limit:_ https://datascience.stackexchange.com/questions/38745/increasing-spacy-max-nlp-limit, hvor der stilles forslag om 1. at hæve værdien i funktionen _nlp.max_length_ for eksempel nlp.max_length = 4000000 og 2. at tilføje til nlp funktionen et argument, der deaktiverer ner og parser fra det obejct, som nlp functionen returnerer. \n",
    "nlp(text, disable = ['ner', 'parser'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff8cac35",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "# Load Danish spacy model\n",
    "nlp = spacy.load(\"da_core_news_sm\")\n",
    "nlp.max_length = 5000000 #or any large value, as long as you don't run out of RAM\n",
    "\n",
    "def lemmatize_text(text):\n",
    "    doc = nlp(text, disable = ['ner', 'parser'])\n",
    "    lemmas = [x.lemma_ for x in doc]\n",
    "    return lemmas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bcdf995",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "startTime = time.time()\n",
    "input_data_grouped['Lemmatized_text'] = input_data_grouped['Clean_text_wo_sw'].progress_apply(lambda x : lemmatize_text(x))\n",
    "executionTime = (time.time() - startTime)\n",
    "print('Execution time in seconds: ' + str(executionTime))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb4dd55a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def join_list(word_list):\n",
    "    return ' '.join(word_list)\n",
    "input_data_grouped['Lemmatized_text'] = input_data_grouped['Lemmatized_text'].apply( lambda x : join_list(x)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f2a1642",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data_grouped[['samling', 'Party','Lemmatized_text']].to_csv('lemmas_tf_df.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9fd59e2",
   "metadata": {},
   "source": [
    "# Tf-Idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca5ba64e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from urllib.request import urlopen\n",
    "\n",
    "# load stopword list from sciencedata.dk\n",
    "with urlopen('https://sciencedata.dk/shared/01f12a3094769a0a6c66fdd0d7bb7dac?download') as response:\n",
    "    stop_words = response.read().decode('utf-8-sig').split('\\r\\n') \n",
    "\n",
    "def tf_idf(text_list):\n",
    "    # Initialize the TF-IDF vectorizer\n",
    "    tfidf_vectorizer = TfidfVectorizer(max_features=10000)\n",
    "\n",
    "    # Fit and transform the text data to calculate TF-IDF scores\n",
    "    tfidf_matrix = tfidf_vectorizer.fit_transform(text_list)\n",
    "\n",
    "    # Get the TF-IDF scores for each item in the text_list\n",
    "    tfidf_scores = tfidf_matrix.toarray()\n",
    "\n",
    "    # Initialize a list to store the TF-IDF scores for each item\n",
    "    tfidf_scores_list = []\n",
    "\n",
    "    # Iterate through the text_list and build the list of TF-IDF scores\n",
    "    for i, item in enumerate(text_list):\n",
    "        words = tfidf_vectorizer.get_feature_names_out()\n",
    "        scores = tfidf_scores[i]\n",
    "        item_scores = {word: score for word, score in zip(words, scores) if score > 0}\n",
    "        tfidf_scores_list.append(item_scores)\n",
    "\n",
    "    return tfidf_scores_list\n",
    "\n",
    "text_list = input_data_grouped['Lemmatized_text'].tolist()\n",
    "\n",
    "\n",
    "distinctive_words = tf_idf(text_list)\n",
    "input_data_grouped['distinctive_words'] =  distinctive_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce77852c",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data_grouped[['samling', 'Party','distinctive_words']].to_csv('distinctive_keywords.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e6c9c73",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data_grouped['distinctive_words']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c83e991",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
