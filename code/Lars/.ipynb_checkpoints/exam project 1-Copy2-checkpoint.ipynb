{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ec34c150",
   "metadata": {},
   "source": [
    "# Exam details\n",
    "Written work:  \n",
    "\n",
    "You will have to analyze a dataset and test a relevant hypothesis.\n",
    "\n",
    "You should choose your own dataset by Lecture 12 (written hand-in already available)\n",
    "\n",
    "The report will be written as a Jupiter notebook.\n",
    "\n",
    "Minimal set of analysis:\n",
    "1) Describe your data and visualize some key dimensions.\n",
    "2) Perform at least two analysis (depending on what is appropriate given the data you selected):\n",
    "- Does your data contain quantitative values that allow for a hypothesis testing?\n",
    "   IF YES: Formulate an hypothesis and test it. Complement the testing with an appropriate visualization.\n",
    "- Does your data contain unstructured textual information?\n",
    " IF YES: Perform sentiment analysis on your data and describe and visualize the results.\n",
    "- Does your data contain network structures (or a network structure can be extracted)?\n",
    " IF YES: Ask a question about the network structure and answer it.\n",
    "\n",
    "OBS: Different datasets can be investigated in many different ways. Any combination of the above-described analysis is acceptable as long as you ask 2 questions. \n",
    "e.g. Statistical hypothesis testing + network analysis, Sentiment analysis + network analysis , Statistical hypothesis testing + Statistical hypothesis testing.\n",
    "\n",
    "Groups:  Find your own group (2 ppls - 3 is possible) let the TAs know before Fall break.\n",
    "\n",
    "Where to find the data:\n",
    "The internet is full of data, these are just few starting points:\n",
    "Data (digst.dk) - Danish Open Data\n",
    "GHO | By theme (who.int) - WHO Open Data\n",
    "DataHub  - Many datasets\n",
    "Data.gov Home - Data.gov - US Open Data\n",
    "Kaggle - All sort of datasets\n",
    "Dataset Search (google.com)\n",
    "TLC Trip Record Data - TLC (nyc.gov) - TAXI data in NYC\n",
    "Industry data and insights | BFI - Movies!\n",
    "CDE (cjis.gov) - Crime data from FBI\n",
    "Københavns Kommune (opendata.dk) - Open Data from CPH"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7af56179",
   "metadata": {},
   "source": [
    "Opgaven skal indeholde en eller flere af følgende: \n",
    "- Hypothesis testing\n",
    "- Sentiment Analyse\n",
    "- Network Analysis \n",
    "\n",
    "Desuden også\n",
    "- Interactive Visualization "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff238b11",
   "metadata": {},
   "source": [
    "# Data\n",
    "Datasættet stammer fra \"The Danish Parliament Corpus 2009 - 2017, v2, w. subject annotation\".\n",
    "\n",
    "Kilde: \n",
    "Hansen, Dorte Haltrup and Navarretta, Costanza, 2021, The Danish Parliament Corpus 2009 - 2017, v2, w. subject annotation, CLARIN-DK-UCPH Centre Repository, http://hdl.handle.net/20.500.12115/44.\n",
    "\n",
    "\n",
    "Datasættet består af transkriptioner af taler i Folketinget fra første samling 2009 til og med først samling 2016 (6/10 2009 – 7/9 2017). Til hver tale er der tilknyttet metadata, dels om medlemmet af folketinget ('Name', 'Gender', 'Party', 'Role', 'Title', 'Birth', 'Age'), dels om talen (Date', 'samling', 'Start time', 'End time', 'Time', 'Agenda item', 'Case no', 'Case type', 'Agenda title', 'Subject 1', 'Subject 2').\n",
    "\n",
    "Datasættet er struktureret i tsv txt-filer, som er formateret i utf-8. Der er en fil per møde.\n",
    "\n",
    "Kilde:\n",
    "Samme, Readme\n",
    "\n",
    "\n",
    "Til denne opgave har vi samlet tsv filerne i et nyt datasæt, som vi har gemt i en csv fil separeret med pipes. Csv filen er uploadet til sciencedata.dk, hvorfra den kan downloades via url med pandas.read_csv() metoden.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Emnet\n",
    "Emnet er immigrationspolitik fra 2009 - 2017. Hvilke kendetegn har de forskellige partiers politik vurdereret ud fra partimedlemers taler i Folketinget?\n",
    "\n",
    "1. Ved hjælp af Tf-Idf identificerer vi de særegne nøgleord, der kendetegner de forskellige partier.\n",
    "\n",
    "2. sentiment analyse på taler som indeholder noget om flygtning. Det kan f.eks. være taler, som handler om 'os' og 'dem' - nærlæsning. Bliver der større variation i sentiment-scorrerne mere varieret op til et valg? Bliver de mere varieret omkring 2015?  \n",
    "3. pos -tag f.eks. verber fra forskellige partier, og hvilke adjektiver knytters sig til et begreb. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04033913",
   "metadata": {},
   "source": [
    "# Load data\n",
    "\n",
    "Data ligger på sciencedata.dk og deles derfra med download link."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "68f61dcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a02c3f08",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('https://sciencedata.dk/shared/825e999a5c13fd22d28d4289fa899ba1?download', sep='|')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8793c5e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The coloumns are Index(['ID', 'Date', 'samling', 'Start time', 'End time', 'Time',\n",
      "       'Agenda item', 'Case no', 'Case type', 'Agenda title', 'Subject 1',\n",
      "       'Subject 2', 'Name', 'Gender', 'Party', 'Role', 'Title', 'Birth', 'Age',\n",
      "       'Text'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print (f'The coloumns are {df.columns}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6c3cf425",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f74f1150",
   "metadata": {},
   "source": [
    "# Question 1\n",
    "# TF-IDF. Segment = speech \n",
    "\n",
    "1. Ved hjælp af Tf-Idf identificerer vi de særegne nøgleord, der kendetegner de forskellige partier.\n",
    "\n",
    "Method:\n",
    "1. Subset data on the subject value \"Immigration\" and on the role value \"member\". Group on session and party and aggregate the speeches according to the groups.  \n",
    "2. Preprocess the texts using a stopword list and ***Spacy*** to lemmatize words in speeches.\n",
    "3. Use **Tf-Idf** to identify distinctive keywords\n",
    "\n",
    "\n",
    "\n",
    "Der skal renses bedre, fordi spacy lemmatizer løber tør for plads med så mange ord.\n",
    "Jeg kan fjerne ord med tal, tal og ord mindre end to bogstaver.\n",
    "\n",
    "\n",
    "ValueError: [E088] Text of length 1975499 exceeds maximum of 1000000. The parser and NER models require roughly 1GB of temporary memory per 100,000 characters in the input. This means long texts may cause memory allocation errors. If you're not using the parser or NER, it's probably safe to increase the `nlp.max_length` limit. The limit is in number of characters, so you can check whether your inputs are too long by checking `len(text)`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "02f5454e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subset data\n",
    "input_data = df[(df['Subject 1'] == 'Immigration') & (df['Role'] == 'medlem')].reset_index()\n",
    "# Group by 'session' and 'party' and aggregate speeches\n",
    "input_data_grouped = df.groupby(['samling', 'Party'])['Text'].agg(' '.join).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ace0c25e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "speeches\n",
      "clean_strings_in_list\n",
      "strings_wo_stop_words\n",
      "strings\n"
     ]
    }
   ],
   "source": [
    "# Preprocess\n",
    "import re\n",
    "\n",
    "def scrub_text(text):\n",
    "    return re.findall(r'\\b\\S+\\b', text.lower().replace('_', ' '))\n",
    "\n",
    "# load stopword list\n",
    "with open('dk.txt', 'r', encoding='utf-8-sig') as f:\n",
    "    stop_words = f.read().split('\\n')    \n",
    "    \n",
    "def filter_stopword(text_list):\n",
    "    return [i for i in text_list if i not in stop_words]\n",
    "  \n",
    "print('speeches')    \n",
    "speeches = input_data_grouped['Text'].tolist()\n",
    "\n",
    "print('clean_strings_in_list')\n",
    "clean_strings_in_list = [scrub_text(i) for i in speeches]\n",
    "\n",
    "print('strings_wo_stop_words')\n",
    "strings_wo_stop_words = [filter_stopword(text) for text in clean_strings_in_list] \n",
    "\n",
    "print('strings')\n",
    "strings = [' '.join(i) for i in strings_wo_stop_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "30c8ec46",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data_grouped['Clean_text_wo_sw'] = strings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efa2827c",
   "metadata": {},
   "source": [
    "Hvis jeg kører lemmatizer koden nedenfor får jeg:\n",
    "\n",
    "_ValueError: [E088] Text of length 1975499 exceeds maximum of 1000000. The parser and NER models require roughly 1GB of temporary memory per 100,000 characters in the input. This means long texts may cause memory allocation errors. If you're not using the parser or NER, it's probably safe to increase the `nlp.max_length` limit. The limit is in number of characters, so you can check whether your inputs are too long by checking `len(text)`._\n",
    "\n",
    "Derfor skal jeg koorigere planen, opdele data i mindre subsets og køre funktionen på mindre dele."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "35f1c08a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>samling</th>\n",
       "      <th>Party</th>\n",
       "      <th>Text</th>\n",
       "      <th>Clean_text_wo_sw</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20091</td>\n",
       "      <td>DF</td>\n",
       "      <td>Tak. Det er jo lidt vanskeligt sådan lige at f...</td>\n",
       "      <td>vanskeligt socialdemokratiet foregået periode ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20091</td>\n",
       "      <td>EL</td>\n",
       "      <td>Det er svært at se, at det er alle i samfundet...</td>\n",
       "      <td>svært samfundet bidrage virker regeringen opfa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>20091</td>\n",
       "      <td>IA</td>\n",
       "      <td>I sensationspressen og på Facebook vrimler det...</td>\n",
       "      <td>sensationspressen facebook vrimler opfordringe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>20091</td>\n",
       "      <td>KD</td>\n",
       "      <td>Tak for det. Vi kender ikke dybden, og vi kend...</td>\n",
       "      <td>kender dybden kender omfanget finanskrisen sta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20091</td>\n",
       "      <td>KF</td>\n",
       "      <td>Det har været meget interessant at følge debat...</td>\n",
       "      <td>interessant følge debatten dag socialdemokrati...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115</th>\n",
       "      <td>20161</td>\n",
       "      <td>S</td>\n",
       "      <td>Tak for det. Beslutningsforslaget, som vi beha...</td>\n",
       "      <td>beslutningsforslaget behandler skær genudsende...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116</th>\n",
       "      <td>20161</td>\n",
       "      <td>SF</td>\n",
       "      <td>Tak for det. Jeg hæftede mig ved ministerens o...</td>\n",
       "      <td>hæftede ministerens opfattelse stiller krav un...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>117</th>\n",
       "      <td>20161</td>\n",
       "      <td>T</td>\n",
       "      <td>Tak. Hr. Søren Espersen, jeg vil bare sige til...</td>\n",
       "      <td>søren espersen velkommen debattere færøerne fa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>118</th>\n",
       "      <td>20161</td>\n",
       "      <td>UFG</td>\n",
       "      <td>Tak for det meget grundige ordførerindlæg, som...</td>\n",
       "      <td>grundige ordførerindlæg socialdemokratiet glad...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119</th>\n",
       "      <td>20161</td>\n",
       "      <td>V</td>\n",
       "      <td>Socialistisk Folkeparti foreslår med det her b...</td>\n",
       "      <td>socialistisk folkeparti foreslår beslutningsfo...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>120 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     samling Party                                               Text  \\\n",
       "0      20091    DF  Tak. Det er jo lidt vanskeligt sådan lige at f...   \n",
       "1      20091    EL  Det er svært at se, at det er alle i samfundet...   \n",
       "2      20091    IA  I sensationspressen og på Facebook vrimler det...   \n",
       "3      20091    KD  Tak for det. Vi kender ikke dybden, og vi kend...   \n",
       "4      20091    KF  Det har været meget interessant at følge debat...   \n",
       "..       ...   ...                                                ...   \n",
       "115    20161     S  Tak for det. Beslutningsforslaget, som vi beha...   \n",
       "116    20161    SF  Tak for det. Jeg hæftede mig ved ministerens o...   \n",
       "117    20161     T  Tak. Hr. Søren Espersen, jeg vil bare sige til...   \n",
       "118    20161   UFG  Tak for det meget grundige ordførerindlæg, som...   \n",
       "119    20161     V  Socialistisk Folkeparti foreslår med det her b...   \n",
       "\n",
       "                                      Clean_text_wo_sw  \n",
       "0    vanskeligt socialdemokratiet foregået periode ...  \n",
       "1    svært samfundet bidrage virker regeringen opfa...  \n",
       "2    sensationspressen facebook vrimler opfordringe...  \n",
       "3    kender dybden kender omfanget finanskrisen sta...  \n",
       "4    interessant følge debatten dag socialdemokrati...  \n",
       "..                                                 ...  \n",
       "115  beslutningsforslaget behandler skær genudsende...  \n",
       "116  hæftede ministerens opfattelse stiller krav un...  \n",
       "117  søren espersen velkommen debattere færøerne fa...  \n",
       "118  grundige ordførerindlæg socialdemokratiet glad...  \n",
       "119  socialistisk folkeparti foreslår beslutningsfo...  \n",
       "\n",
       "[120 rows x 4 columns]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_data_grouped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "587beed8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "# Load Danish spacy model\n",
    "nlp = spacy.load(\"da_core_news_sm\")\n",
    "\n",
    "\n",
    "def lemmatize_text(text):\n",
    "    # Process the text with spaCy\n",
    "    doc = nlp(text)\n",
    "\n",
    "    # Extract the lemmatized tokens and join them back into a string\n",
    "    lemmatized_text = \" \".join([token.lemma_ for token in doc])\n",
    "\n",
    "    return lemmatized_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "ebaf1435",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_div_1 = input_data_grouped.iloc[0:20, [0, 1, 3]]\n",
    "input_div_2 = input_data_grouped.iloc[20:40, [0, 1, 3]]\n",
    "input_div_3 = input_data_grouped.iloc[40:60, [0, 1, 3]]\n",
    "input_div_4 = input_data_grouped.iloc[60:80, [0, 1, 3]]\n",
    "input_div_5 = input_data_grouped.iloc[80:100, [0, 1, 3]]\n",
    "input_div_6 = input_data_grouped.iloc[100:120, [0, 1, 3]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "9496c32a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>samling</th>\n",
       "      <th>Party</th>\n",
       "      <th>Clean_text_wo_sw</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20091</td>\n",
       "      <td>DF</td>\n",
       "      <td>vanskeligt socialdemokratiet foregået periode ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20091</td>\n",
       "      <td>EL</td>\n",
       "      <td>svært samfundet bidrage virker regeringen opfa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>20091</td>\n",
       "      <td>IA</td>\n",
       "      <td>sensationspressen facebook vrimler opfordringe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>20091</td>\n",
       "      <td>KD</td>\n",
       "      <td>kender dybden kender omfanget finanskrisen sta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20091</td>\n",
       "      <td>KF</td>\n",
       "      <td>interessant følge debatten dag socialdemokrati...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>20091</td>\n",
       "      <td>LA</td>\n",
       "      <td>fremmest fint mulighed stemme ja 4-5 uger libe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>20091</td>\n",
       "      <td>RV</td>\n",
       "      <td>venstres ordfører startede regningen betalt ri...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>20091</td>\n",
       "      <td>S</td>\n",
       "      <td>formand hemmelighed socialdemokratiet usympati...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>20091</td>\n",
       "      <td>SF</td>\n",
       "      <td>mødet åbnet finansudvalget afgivet betænkning ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>20091</td>\n",
       "      <td>SIU</td>\n",
       "      <td>indledningsvis fraråde tale sælge hinandens la...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>20091</td>\n",
       "      <td>SP</td>\n",
       "      <td>folketingets afslutningsdebat billede situatio...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>20091</td>\n",
       "      <td>TF</td>\n",
       "      <td>forstod morgen statsministeren planer flytte f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>20091</td>\n",
       "      <td>UFG</td>\n",
       "      <td>formand handler mariehønens mulighed fri bevæg...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>20091</td>\n",
       "      <td>V</td>\n",
       "      <td>regeringen sammen dansk folkeparti indgået ans...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>20101</td>\n",
       "      <td>DF</td>\n",
       "      <td>spørge bjarne laustsen stor respekt samarbejde...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>20101</td>\n",
       "      <td>EL</td>\n",
       "      <td>følte trang blande debatten ændringsforslaget ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>20101</td>\n",
       "      <td>IA</td>\n",
       "      <td>debatten dag markerer afslutningen min tid med...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>20101</td>\n",
       "      <td>KD</td>\n",
       "      <td>hører statsministeren sammenligne bygning inve...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>20101</td>\n",
       "      <td>KF</td>\n",
       "      <td>yderligere bemærkning marianne jelved integrat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>20101</td>\n",
       "      <td>LA</td>\n",
       "      <td>enig henrik dam kristensen dårligt lovforslag ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    samling Party                                   Clean_text_wo_sw\n",
       "0     20091    DF  vanskeligt socialdemokratiet foregået periode ...\n",
       "1     20091    EL  svært samfundet bidrage virker regeringen opfa...\n",
       "2     20091    IA  sensationspressen facebook vrimler opfordringe...\n",
       "3     20091    KD  kender dybden kender omfanget finanskrisen sta...\n",
       "4     20091    KF  interessant følge debatten dag socialdemokrati...\n",
       "5     20091    LA  fremmest fint mulighed stemme ja 4-5 uger libe...\n",
       "6     20091    RV  venstres ordfører startede regningen betalt ri...\n",
       "7     20091     S  formand hemmelighed socialdemokratiet usympati...\n",
       "8     20091    SF  mødet åbnet finansudvalget afgivet betænkning ...\n",
       "9     20091   SIU  indledningsvis fraråde tale sælge hinandens la...\n",
       "10    20091    SP  folketingets afslutningsdebat billede situatio...\n",
       "11    20091    TF  forstod morgen statsministeren planer flytte f...\n",
       "12    20091   UFG  formand handler mariehønens mulighed fri bevæg...\n",
       "13    20091     V  regeringen sammen dansk folkeparti indgået ans...\n",
       "14    20101    DF  spørge bjarne laustsen stor respekt samarbejde...\n",
       "15    20101    EL  følte trang blande debatten ændringsforslaget ...\n",
       "16    20101    IA  debatten dag markerer afslutningen min tid med...\n",
       "17    20101    KD  hører statsministeren sammenligne bygning inve...\n",
       "18    20101    KF  yderligere bemærkning marianne jelved integrat...\n",
       "19    20101    LA  enig henrik dam kristensen dårligt lovforslag ..."
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_div_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "36ff9927",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "[E088] Text of length 1975499 exceeds maximum of 1000000. The parser and NER models require roughly 1GB of temporary memory per 100,000 characters in the input. This means long texts may cause memory allocation errors. If you're not using the parser or NER, it's probably safe to increase the `nlp.max_length` limit. The limit is in number of characters, so you can check whether your inputs are too long by checking `len(text)`.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[39], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m input_div_1[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLemmatized_text\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43minput_div_1\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mClean_text_wo_sw\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlemmatize_text\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\lib\\site-packages\\pandas\\core\\series.py:4765\u001b[0m, in \u001b[0;36mSeries.apply\u001b[1;34m(self, func, convert_dtype, args, by_row, **kwargs)\u001b[0m\n\u001b[0;32m   4625\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply\u001b[39m(\n\u001b[0;32m   4626\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   4627\u001b[0m     func: AggFuncType,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   4632\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m   4633\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame \u001b[38;5;241m|\u001b[39m Series:\n\u001b[0;32m   4634\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   4635\u001b[0m \u001b[38;5;124;03m    Invoke function on values of Series.\u001b[39;00m\n\u001b[0;32m   4636\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   4756\u001b[0m \u001b[38;5;124;03m    dtype: float64\u001b[39;00m\n\u001b[0;32m   4757\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m   4758\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mSeriesApply\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   4759\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4760\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4761\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconvert_dtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconvert_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4762\u001b[0m \u001b[43m        \u001b[49m\u001b[43mby_row\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mby_row\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4763\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4764\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m-> 4765\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\lib\\site-packages\\pandas\\core\\apply.py:1201\u001b[0m, in \u001b[0;36mSeriesApply.apply\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1198\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_compat()\n\u001b[0;32m   1200\u001b[0m \u001b[38;5;66;03m# self.func is Callable\u001b[39;00m\n\u001b[1;32m-> 1201\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_standard\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\lib\\site-packages\\pandas\\core\\apply.py:1281\u001b[0m, in \u001b[0;36mSeriesApply.apply_standard\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1275\u001b[0m \u001b[38;5;66;03m# row-wise access\u001b[39;00m\n\u001b[0;32m   1276\u001b[0m \u001b[38;5;66;03m# apply doesn't have a `na_action` keyword and for backward compat reasons\u001b[39;00m\n\u001b[0;32m   1277\u001b[0m \u001b[38;5;66;03m# we need to give `na_action=\"ignore\"` for categorical data.\u001b[39;00m\n\u001b[0;32m   1278\u001b[0m \u001b[38;5;66;03m# TODO: remove the `na_action=\"ignore\"` when that default has been changed in\u001b[39;00m\n\u001b[0;32m   1279\u001b[0m \u001b[38;5;66;03m#  Categorical (GH51645).\u001b[39;00m\n\u001b[0;32m   1280\u001b[0m action \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(obj\u001b[38;5;241m.\u001b[39mdtype, CategoricalDtype) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1281\u001b[0m mapped \u001b[38;5;241m=\u001b[39m \u001b[43mobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_map_values\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1282\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmapper\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcurried\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mna_action\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maction\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert_dtype\u001b[49m\n\u001b[0;32m   1283\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1285\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(mapped) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(mapped[\u001b[38;5;241m0\u001b[39m], ABCSeries):\n\u001b[0;32m   1286\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m   1287\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mReturning a DataFrame from Series.apply when the supplied function \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1288\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreturns a Series is deprecated and will be removed in a future \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1291\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39mfind_stack_level(),\n\u001b[0;32m   1292\u001b[0m     )  \u001b[38;5;66;03m# GH52116\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\lib\\site-packages\\pandas\\core\\base.py:921\u001b[0m, in \u001b[0;36mIndexOpsMixin._map_values\u001b[1;34m(self, mapper, na_action, convert)\u001b[0m\n\u001b[0;32m    918\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(arr, ExtensionArray):\n\u001b[0;32m    919\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m arr\u001b[38;5;241m.\u001b[39mmap(mapper, na_action\u001b[38;5;241m=\u001b[39mna_action)\n\u001b[1;32m--> 921\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43malgorithms\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43marr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmapper\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mna_action\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mna_action\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\lib\\site-packages\\pandas\\core\\algorithms.py:1812\u001b[0m, in \u001b[0;36mmap_array\u001b[1;34m(arr, mapper, na_action, convert)\u001b[0m\n\u001b[0;32m   1810\u001b[0m values \u001b[38;5;241m=\u001b[39m arr\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mobject\u001b[39m, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m   1811\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m na_action \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1812\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap_infer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmapper\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1813\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1814\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m lib\u001b[38;5;241m.\u001b[39mmap_infer_mask(\n\u001b[0;32m   1815\u001b[0m         values, mapper, mask\u001b[38;5;241m=\u001b[39misna(values)\u001b[38;5;241m.\u001b[39mview(np\u001b[38;5;241m.\u001b[39muint8), convert\u001b[38;5;241m=\u001b[39mconvert\n\u001b[0;32m   1816\u001b[0m     )\n",
      "File \u001b[1;32mlib.pyx:2917\u001b[0m, in \u001b[0;36mpandas._libs.lib.map_infer\u001b[1;34m()\u001b[0m\n",
      "Cell \u001b[1;32mIn[39], line 1\u001b[0m, in \u001b[0;36m<lambda>\u001b[1;34m(x)\u001b[0m\n\u001b[1;32m----> 1\u001b[0m input_div_1[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLemmatized_text\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m input_div_1[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mClean_text_wo_sw\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m x : \u001b[43mlemmatize_text\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m)\n",
      "Cell \u001b[1;32mIn[24], line 9\u001b[0m, in \u001b[0;36mlemmatize_text\u001b[1;34m(text)\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mlemmatize_text\u001b[39m(text):\n\u001b[0;32m      8\u001b[0m     \u001b[38;5;66;03m# Process the text with spaCy\u001b[39;00m\n\u001b[1;32m----> 9\u001b[0m     doc \u001b[38;5;241m=\u001b[39m \u001b[43mnlp\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     11\u001b[0m     \u001b[38;5;66;03m# Extract the lemmatized tokens and join them back into a string\u001b[39;00m\n\u001b[0;32m     12\u001b[0m     lemmatized_text \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin([token\u001b[38;5;241m.\u001b[39mlemma_ \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m doc])\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\lib\\site-packages\\spacy\\language.py:999\u001b[0m, in \u001b[0;36mLanguage.__call__\u001b[1;34m(self, text, disable, component_cfg)\u001b[0m\n\u001b[0;32m    978\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\n\u001b[0;32m    979\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    980\u001b[0m     text: Union[\u001b[38;5;28mstr\u001b[39m, Doc],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    983\u001b[0m     component_cfg: Optional[Dict[\u001b[38;5;28mstr\u001b[39m, Dict[\u001b[38;5;28mstr\u001b[39m, Any]]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    984\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Doc:\n\u001b[0;32m    985\u001b[0m     \u001b[38;5;124;03m\"\"\"Apply the pipeline to some text. The text can span multiple sentences,\u001b[39;00m\n\u001b[0;32m    986\u001b[0m \u001b[38;5;124;03m    and can contain arbitrary whitespace. Alignment into the original string\u001b[39;00m\n\u001b[0;32m    987\u001b[0m \u001b[38;5;124;03m    is preserved.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    997\u001b[0m \u001b[38;5;124;03m    DOCS: https://spacy.io/api/language#call\u001b[39;00m\n\u001b[0;32m    998\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 999\u001b[0m     doc \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_ensure_doc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1000\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m component_cfg \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1001\u001b[0m         component_cfg \u001b[38;5;241m=\u001b[39m {}\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\lib\\site-packages\\spacy\\language.py:1090\u001b[0m, in \u001b[0;36mLanguage._ensure_doc\u001b[1;34m(self, doc_like)\u001b[0m\n\u001b[0;32m   1088\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m doc_like\n\u001b[0;32m   1089\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(doc_like, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m-> 1090\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmake_doc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdoc_like\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1091\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(doc_like, \u001b[38;5;28mbytes\u001b[39m):\n\u001b[0;32m   1092\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Doc(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvocab)\u001b[38;5;241m.\u001b[39mfrom_bytes(doc_like)\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\lib\\site-packages\\spacy\\language.py:1079\u001b[0m, in \u001b[0;36mLanguage.make_doc\u001b[1;34m(self, text)\u001b[0m\n\u001b[0;32m   1073\u001b[0m \u001b[38;5;124;03m\"\"\"Turn a text into a Doc object.\u001b[39;00m\n\u001b[0;32m   1074\u001b[0m \n\u001b[0;32m   1075\u001b[0m \u001b[38;5;124;03mtext (str): The text to process.\u001b[39;00m\n\u001b[0;32m   1076\u001b[0m \u001b[38;5;124;03mRETURNS (Doc): The processed doc.\u001b[39;00m\n\u001b[0;32m   1077\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1078\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(text) \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_length:\n\u001b[1;32m-> 1079\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1080\u001b[0m         Errors\u001b[38;5;241m.\u001b[39mE088\u001b[38;5;241m.\u001b[39mformat(length\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m(text), max_length\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_length)\n\u001b[0;32m   1081\u001b[0m     )\n\u001b[0;32m   1082\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokenizer(text)\n",
      "\u001b[1;31mValueError\u001b[0m: [E088] Text of length 1975499 exceeds maximum of 1000000. The parser and NER models require roughly 1GB of temporary memory per 100,000 characters in the input. This means long texts may cause memory allocation errors. If you're not using the parser or NER, it's probably safe to increase the `nlp.max_length` limit. The limit is in number of characters, so you can check whether your inputs are too long by checking `len(text)`."
     ]
    }
   ],
   "source": [
    "input_div_1['Lemmatized_text'] = input_div_1['Clean_text_wo_sw'].apply(lambda x : lemmatize_text(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "6aefcfd6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1975499"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(input_div_1.at[0,'Clean_text_wo_sw'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67a2f23c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19f3a799",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "177ed109",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "211f8b45",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# load stopword list\n",
    "with open('dk.txt', 'r', encoding='utf-8-sig') as f:\n",
    "    stop_words = f.read().split('\\n')\n",
    "\n",
    "def top_distinctive_words(documents):\n",
    "    # Create a TF-IDF vectorizer\n",
    "    tfidf_vectorizer = TfidfVectorizer(max_features=10000)\n",
    "\n",
    "    # Fit and transform the input documents\n",
    "    tfidf_matrix = tfidf_vectorizer.fit_transform(documents)\n",
    "\n",
    "    # Get the feature names (words)\n",
    "    feature_names = tfidf_vectorizer.get_feature_names_out()\n",
    "\n",
    "    # Initialize a list to store the top distinctive words for each document\n",
    "    top_words_list = []\n",
    "\n",
    "    # Iterate through the TF-IDF matrices for each document\n",
    "    for tfidf_vector in tfidf_matrix.toarray():\n",
    "        # Create a list of tuples (word, TF-IDF score) for the current document\n",
    "        word_tfidf_tuples = [(feature, tfidf_score) for feature, tfidf_score in zip(feature_names, tfidf_vector) if tfidf_score > 0]\n",
    "\n",
    "        # Sort the tuples by TF-IDF score in descending order\n",
    "        word_tfidf_tuples.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "        # Select the top five words with the highest TF-IDF scores\n",
    "        top_words = [word for word, _ in word_tfidf_tuples[:20]]\n",
    "\n",
    "        top_words_list.append(top_words)\n",
    "\n",
    "    return top_words_list\n",
    "\n",
    "\n",
    "\n",
    "documents = input_data_grouped['Text'].tolist()\n",
    "\n",
    "\n",
    "top_words_list = top_distinctive_words(documents)\n",
    "\n",
    "input_data_grouped['distinctive_keywords'] =  top_words_list\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "357514ae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73a7b833",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e46b728",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eece60f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c01c43c0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "490590f1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4744d8d6",
   "metadata": {},
   "source": [
    "# Question 3\n",
    "\n",
    "Sentiment analyse af taler som indeholder noget om flygtning. Det kan f.eks. være taler, som handler om 'os' og 'dem' - nærlæsning. Bliver der større variation i sentiment-scorerne mere varieret op til et valg? Bliver de mere varieret omkring 2015?\n",
    "\n",
    "M: Brug sentiment analyse til at identificere forandringer i sentimenter i taler knyttet til emneordet Immigration.\n",
    "Definition: \"SA is a part of applied computational linguistics and attempts to quantify the emotions\" (Kran, E., & Orm, S. (2020)).\n",
    "\n",
    "\n",
    "### Hvilken sentiment analyse tilgang anvender vi?\n",
    "\n",
    "På Alexandra Instituttets DaNLP repository, et repository for Natural Language Processing resources for the Danish Language, leverer Alexandra Instituttet et overblik over open sentiment analysis models and dataset for Danish.\n",
    "\n",
    "Der er følgende modeller:\n",
    "1. AFINN - wordlist model, der returnerer en tal-score, der modsvarer en følelse i et ord. Negativ ( minus ), neutral (0) eller positiv ( plus ) \n",
    "2. Sentida - wordlist model, der ligesom ovenfor returnerer en tal-score, der modsvarer en følelse.\n",
    "3. BERT Emotion - BERT model, der returner en beskrivende tekststreng, der modsvarer et semantisk felt, som et ord er indlejret i, f.eks. glæde/sindsro, forventning/interesse, tillid/accept.     \n",
    "4. BERT Tone - Bert model, der ligeledes returner en beskrivende tekststreng.\n",
    "5. SpaCy Sentiment - Spacy model, der også returner en tekststreng.\n",
    "6. Senda - Bert model, der også returner en tekststreng. \n",
    "\n",
    "De to første, AFINN og Sentida, samt den sidste Senda, er ikke en del af danlp projektet og dermed ikke en del af DaNLP Python projektet, som kan anvendes via pip. \n",
    "\n",
    "Kilde: Alexandra Institute. (2021). Sentiment_analysis.md. https://github.com/alexandrainst/danlp/blob/master/docs/docs/tasks/sentiment_analysis.md\n",
    "\n",
    "\n",
    "I denne opgave vil vi anvende sentiment analyse til at se på variationer i scorerne. Vi kan derfor vælge en af de første to, hvorved vi også vælger ikke at anvende DaNLPs Python pakke. Af de to første wordlist modeller, AFINN og Sentida vælger vi Sentida, fordi modellen er blevet opdateret med ny funktionalitet, og fordi set ud fra Finn Årup Nielsens publicationer siden 2019 har han ikke udgivet nyt omkring sentiment analyse i den periode. \n",
    "\n",
    "Både AFINN og Sentida er wordlist modeller, der aggregerer en sentiment scores baseret på forekomsten af ord fra ordlisten i en given tekst. Den aggregerede score bliver anvendt som en indikator på, hvor positiv teksten \n",
    "er. Der er fire problemer forbundet med denne model:\n",
    "1. modellen tager ikke højde for syntaktiske relation mellem ord.\n",
    "2. modellen ignorerer adverbier, og dermed den betydning adverbierne har i at udtykke grader af noget og holdinger til noget.\n",
    "3. modellen afspejler ikke menneskers måde at opfatte følelser.\n",
    "4. modellen kan ikke håndtere ord der betyder to forskellige ting.\n",
    "Udviklerne bag Sentida har fundet inspiration i den engelsk sprogede VADER model og har forsøgt at minimere problemerne ved at indbygge forskellige, simple former for \"awareness\". For eksempel at forøge score omkring negationer, captital letter og udråbstegn for at give disse elementer i sproget større opmærksomhed. \n",
    "\n",
    "For example:\n",
    "\n",
    "_“Maden (+0.3) var god (+2.3), (← x 0.5) men(1.5 x →) serviceringen (+0.3) var elendig (-4.3).” ⇒ 1.3 -6 ⇒ sentiment score: -4.7 \n",
    "\n",
    "“The  food (+0.3)was  good (+2.3),  (←  x  0.5) but(1.5x  →) the  service (+0.3)was horrendous (-4.3).” ⇒ 1.3 -6 ⇒ sentiment score: -4.7_\n",
    "\n",
    "\n",
    "_“Det er så sejt (+3.6)!(← x 1.291)” ⇒ sentiment score: 4.6\n",
    "\n",
    "“It is so cool (+3.6)!(←x 1.291)”⇒ sentiment score: 4.6_\n",
    "\n",
    "\n",
    "_“DET ERSÅ SEJT (+3.6). (← x 1.733)” ⇒ sentiment score: 6.2\n",
    "\n",
    "“IT IS SO COOL (+3.6). (← x 1.733)” ⇒ sentiment score: 6.2_\n",
    "\n",
    "\n",
    "(Kran, E., & Orm, S. (2020)).\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Neural networks baserede SA modeller som \"aspect-based sentiment analysis\" kan tage højde for ord og kontekst og er på den måde mere præcise, men eftersom neurale networds modeller forudsætter store mængder af træningsdata, er de langsomme og upraktiske at benytte (Kran, E., & Orm, S. (2020))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3878325a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install sentida"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63704f54",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentida import Sentida\n",
    "# Define the class:\n",
    "SV = Sentida()\n",
    "\n",
    "SV.sentida(\n",
    "        text = 'Lad der blive fred.',\n",
    "        output = 'mean',\n",
    "        normal = False,\n",
    "        speed = 'normal')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52a97d2f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b11397dc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c3cd481",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46610c7d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dca251e3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abdb8f4d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a195277",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c0871128",
   "metadata": {},
   "source": [
    "https://github.com/alexandrainst/danlp/blob/master/docs/docs/tasks/sentiment_analysis.md\n",
    "\n",
    "https://github.com/alexandrainst/danlp/blob/master/docs/docs/frameworks/spacy.md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7347ce7d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3de80476",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "240a4ed8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92114b41",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6ba62045",
   "metadata": {},
   "source": [
    "# Litteraturliste\n",
    "\n",
    "Lauridsen, G. A., Dalsgaard, J. A., & Svendsen, L. K. B. (2019). SENTIDA: A New Tool for Sentiment Analysis in Danish. Journal of Language Works - Sprogvidenskabeligt Studentertidsskrift, 4(1), 38–53. Retrieved from https://tidsskrift.dk/lwo/article/view/115711\n",
    "\n",
    "Kran, E., & Orm, S. (2020). EMMA: Danish Natural-Language Processing of Emotion in Text: The new State-of-the-Art in Danish Sentiment Analysis and a Multidimensional Emotional Sentiment Validation Dataset. Journal of Language Works - Sprogvidenskabeligt Studentertidsskrift, 5(1), 92–110. Retrieved from https://tidsskrift.dk/lwo/article/view/121221\n",
    "\n",
    "Brogaard Pauli, Amalie and Barrett, Maria and Lacroix, Ophélie and Hvingelby, Rasmus. (2021) An open-source toolkit for Danish Natural Language Processing. https://ep.liu.se/ecp/178/053/ecp2021178053.pdf \n",
    "\n",
    "Alexandra Institute. (2021). Sentiment_analysis.md. https://github.com/alexandrainst/danlp/blob/master/docs/docs/tasks/sentiment_analysis.md\n",
    "\n",
    "Hansen, Dorte Haltrup and Navarretta, Costanza, 2021, The Danish Parliament Corpus 2009 - 2017, v2, w. subject annotation, CLARIN-DK-UCPH Centre Repository, http://hdl.handle.net/20.500.12115/44.\n",
    "\n",
    "McKinney, Wes. Python for Data Analysis : Data Wrangling with Pandas, NumPy and Jupyter. Third edition. Sebastopol, CA: O’Reilly Media, Inc., 2022"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c2e6e64",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61506c2f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20c61943",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "515f192b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# bump_chart\n",
    "# https://altair-viz.github.io/gallery/bump_chart.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79483881",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
