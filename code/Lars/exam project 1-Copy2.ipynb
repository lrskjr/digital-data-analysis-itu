{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ec34c150",
   "metadata": {},
   "source": [
    "# Exam details\n",
    "Written work:  \n",
    "\n",
    "You will have to analyze a dataset and test a relevant hypothesis.\n",
    "\n",
    "You should choose your own dataset by Lecture 12 (written hand-in already available)\n",
    "\n",
    "The report will be written as a Jupiter notebook.\n",
    "\n",
    "Minimal set of analysis:\n",
    "1) Describe your data and visualize some key dimensions.\n",
    "2) Perform at least two analysis (depending on what is appropriate given the data you selected):\n",
    "- Does your data contain quantitative values that allow for a hypothesis testing?\n",
    "   IF YES: Formulate an hypothesis and test it. Complement the testing with an appropriate visualization.\n",
    "- Does your data contain unstructured textual information?\n",
    " IF YES: Perform sentiment analysis on your data and describe and visualize the results.\n",
    "- Does your data contain network structures (or a network structure can be extracted)?\n",
    " IF YES: Ask a question about the network structure and answer it.\n",
    "\n",
    "OBS: Different datasets can be investigated in many different ways. Any combination of the above-described analysis is acceptable as long as you ask 2 questions. \n",
    "e.g. Statistical hypothesis testing + network analysis, Sentiment analysis + network analysis , Statistical hypothesis testing + Statistical hypothesis testing.\n",
    "\n",
    "Groups:  Find your own group (2 ppls - 3 is possible) let the TAs know before Fall break.\n",
    "\n",
    "Where to find the data:\n",
    "The internet is full of data, these are just few starting points:\n",
    "Data (digst.dk) - Danish Open Data\n",
    "GHO | By theme (who.int) - WHO Open Data\n",
    "DataHub  - Many datasets\n",
    "Data.gov Home - Data.gov - US Open Data\n",
    "Kaggle - All sort of datasets\n",
    "Dataset Search (google.com)\n",
    "TLC Trip Record Data - TLC (nyc.gov) - TAXI data in NYC\n",
    "Industry data and insights | BFI - Movies!\n",
    "CDE (cjis.gov) - Crime data from FBI\n",
    "Københavns Kommune (opendata.dk) - Open Data from CPH"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7af56179",
   "metadata": {},
   "source": [
    "Opgaven skal indeholde en eller flere af følgende: \n",
    "- Hypothesis testing\n",
    "- Sentiment Analyse\n",
    "- Network Analysis \n",
    "\n",
    "Desuden også\n",
    "- Interactive Visualization "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff238b11",
   "metadata": {},
   "source": [
    "# Data\n",
    "Datasættet stammer fra \"The Danish Parliament Corpus 2009 - 2017, v2, w. subject annotation\".\n",
    "\n",
    "Kilde: \n",
    "Hansen, Dorte Haltrup and Navarretta, Costanza, 2021, The Danish Parliament Corpus 2009 - 2017, v2, w. subject annotation, CLARIN-DK-UCPH Centre Repository, http://hdl.handle.net/20.500.12115/44.\n",
    "\n",
    "\n",
    "Datasættet består af transkriptioner af taler i Folketinget fra første samling 2009 til og med først samling 2016 (6/10 2009 – 7/9 2017). Til hver tale er der tilknyttet metadata, dels om medlemmet af folketinget ('Name', 'Gender', 'Party', 'Role', 'Title', 'Birth', 'Age'), dels om talen (Date', 'samling', 'Start time', 'End time', 'Time', 'Agenda item', 'Case no', 'Case type', 'Agenda title', 'Subject 1', 'Subject 2').\n",
    "\n",
    "Datasættet er struktureret i tsv txt-filer, som er formateret i utf-8. Der er en fil per møde.\n",
    "\n",
    "Kilde:\n",
    "Samme, Readme\n",
    "\n",
    "\n",
    "Til denne opgave har vi samlet tsv filerne i et nyt datasæt, som vi har gemt i en csv fil separeret med pipes. Csv filen er uploadet til sciencedata.dk, hvorfra den kan downloades via url med pandas.read_csv() metoden.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Emnet\n",
    "Emnet er immigrationspolitik fra 2009 - 2017. Hvilke kendetegn har de forskellige partiers politik vurdereret ud fra partimedlemers taler i Folketinget?\n",
    "\n",
    "1. Ved hjælp af Tf-Idf identificerer vi de særegne nøgleord, der kendetegner de forskellige partier.\n",
    "\n",
    "2. sentiment analyse på taler som indeholder noget om flygtning. Det kan f.eks. være taler, som handler om 'os' og 'dem' - nærlæsning. Bliver der større variation i sentiment-scorrerne mere varieret op til et valg? Bliver de mere varieret omkring 2015?  \n",
    "3. pos -tag f.eks. verber fra forskellige partier, og hvilke adjektiver knytters sig til et begreb. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04033913",
   "metadata": {},
   "source": [
    "# Load data\n",
    "\n",
    "Data ligger på sciencedata.dk og deles derfra med download link."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68f61dcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a02c3f08",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('https://sciencedata.dk/shared/825e999a5c13fd22d28d4289fa899ba1?download', sep='|')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8793c5e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print (f'The coloumns are {df.columns}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c3cf425",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f74f1150",
   "metadata": {},
   "source": [
    "# Question 1\n",
    "# TF-IDF. Segment = speech \n",
    "\n",
    "1. Ved hjælp af Tf-Idf identificerer vi de særegne nøgleord, der kendetegner de forskellige partier.\n",
    "\n",
    "Method:\n",
    "1. Subset data on the subject value \"Immigration\" and on the role value \"member\". Group on session and party and aggregate the speeches according to the groups.  \n",
    "2. Preprocess the texts using a stopword list and ***Spacy*** to lemmatize words in speeches.\n",
    "3. Use **Tf-Idf** to identify distinctive keywords\n",
    "\n",
    "\n",
    "\n",
    "Der skal renses bedre, fordi spacy lemmatizer løber tør for plads med så mange ord.\n",
    "Jeg kan fjerne ord med tal, tal og ord mindre end to bogstaver.\n",
    "\n",
    "\n",
    "ValueError: [E088] Text of length 1975499 exceeds maximum of 1000000. The parser and NER models require roughly 1GB of temporary memory per 100,000 characters in the input. This means long texts may cause memory allocation errors. If you're not using the parser or NER, it's probably safe to increase the `nlp.max_length` limit. The limit is in number of characters, so you can check whether your inputs are too long by checking `len(text)`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02f5454e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subset data\n",
    "input_data = df[(df['Subject 1'] == 'Immigration') & (df['Role'] == 'medlem')].reset_index()\n",
    "# Group by 'session' and 'party' and aggregate speeches\n",
    "input_data_grouped = df.groupby(['samling', 'Party'])['Text'].agg(' '.join).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ace0c25e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess\n",
    "import re\n",
    "\n",
    "def scrub_text(text):\n",
    "    return re.findall(r'\\b\\S+\\b', text.lower().replace('_', ' '))\n",
    "\n",
    "# load stopword list\n",
    "with open('dk.txt', 'r', encoding='utf-8-sig') as f:\n",
    "    stop_words = f.read().split('\\n')    \n",
    "    \n",
    "def filter_stopword(text_list):\n",
    "    return [i for i in text_list if i not in stop_words]\n",
    "  \n",
    "print('speeches')    \n",
    "speeches = input_data_grouped['Text'].tolist()\n",
    "\n",
    "print('clean_strings_in_list')\n",
    "clean_strings_in_list = [scrub_text(i) for i in speeches]\n",
    "\n",
    "print('strings_wo_stop_words')\n",
    "strings_wo_stop_words = [filter_stopword(text) for text in clean_strings_in_list] \n",
    "\n",
    "print('strings')\n",
    "strings = [' '.join(i) for i in strings_wo_stop_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b02e005",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data_grouped['Clean_text_wo_sw'] = strings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a193498",
   "metadata": {},
   "source": [
    "Hvis jeg kører lemmatizer koden nedenfor får jeg:\n",
    "\n",
    "_ValueError: [E088] Text of length 1975499 exceeds maximum of 1000000. The parser and NER models require roughly 1GB of temporary memory per 100,000 characters in the input. This means long texts may cause memory allocation errors. If you're not using the parser or NER, it's probably safe to increase the `nlp.max_length` limit. The limit is in number of characters, so you can check whether your inputs are too long by checking `len(text)`._\n",
    "\n",
    "Derfor skal jeg koorigere planen, opdele data i mindre subsets og køre funktionen på mindre dele."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93ae327e",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data_grouped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "587beed8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "# Load Danish spacy model\n",
    "nlp = spacy.load(\"da_core_news_sm\")\n",
    "\n",
    "\n",
    "def lemmatize_text(text):\n",
    "    # Process the text with spaCy\n",
    "    doc = nlp(text)\n",
    "\n",
    "    # Extract the lemmatized tokens and join them back into a string\n",
    "    lemmatized_text = \" \".join([token.lemma_ for token in doc])\n",
    "\n",
    "    return lemmatized_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ed07500",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_div_1 = input_data_grouped.iloc[0:20, [0, 1, 3]]\n",
    "input_div_2 = input_data_grouped.iloc[20:40, [0, 1, 3]]\n",
    "input_div_3 = input_data_grouped.iloc[40:60, [0, 1, 3]]\n",
    "input_div_4 = input_data_grouped.iloc[60:80, [0, 1, 3]]\n",
    "input_div_5 = input_data_grouped.iloc[80:100, [0, 1, 3]]\n",
    "input_div_6 = input_data_grouped.iloc[100:120, [0, 1, 3]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ef0fab9",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_div_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36ff9927",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_div_1['Lemmatized_text'] = input_div_1['Clean_text_wo_sw'].apply(lambda x : lemmatize_text(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aefcfd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(input_div_1.at[0,'Clean_text_wo_sw'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67a2f23c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19f3a799",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "177ed109",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "211f8b45",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# load stopword list\n",
    "with open('dk.txt', 'r', encoding='utf-8-sig') as f:\n",
    "    stop_words = f.read().split('\\n')\n",
    "\n",
    "def top_distinctive_words(documents):\n",
    "    # Create a TF-IDF vectorizer\n",
    "    tfidf_vectorizer = TfidfVectorizer(max_features=10000)\n",
    "\n",
    "    # Fit and transform the input documents\n",
    "    tfidf_matrix = tfidf_vectorizer.fit_transform(documents)\n",
    "\n",
    "    # Get the feature names (words)\n",
    "    feature_names = tfidf_vectorizer.get_feature_names_out()\n",
    "\n",
    "    # Initialize a list to store the top distinctive words for each document\n",
    "    top_words_list = []\n",
    "\n",
    "    # Iterate through the TF-IDF matrices for each document\n",
    "    for tfidf_vector in tfidf_matrix.toarray():\n",
    "        # Create a list of tuples (word, TF-IDF score) for the current document\n",
    "        word_tfidf_tuples = [(feature, tfidf_score) for feature, tfidf_score in zip(feature_names, tfidf_vector) if tfidf_score > 0]\n",
    "\n",
    "        # Sort the tuples by TF-IDF score in descending order\n",
    "        word_tfidf_tuples.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "        # Select the top five words with the highest TF-IDF scores\n",
    "        top_words = [word for word, _ in word_tfidf_tuples[:20]]\n",
    "\n",
    "        top_words_list.append(top_words)\n",
    "\n",
    "    return top_words_list\n",
    "\n",
    "\n",
    "\n",
    "documents = input_data_grouped['Text'].tolist()\n",
    "\n",
    "\n",
    "top_words_list = top_distinctive_words(documents)\n",
    "\n",
    "input_data_grouped['distinctive_keywords'] =  top_words_list\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "357514ae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73a7b833",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e46b728",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eece60f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c01c43c0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "490590f1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4744d8d6",
   "metadata": {},
   "source": [
    "# Question 3\n",
    "\n",
    "Sentiment analyse af taler som indeholder noget om flygtning. Det kan f.eks. være taler, som handler om 'os' og 'dem' - nærlæsning. Bliver der større variation i sentiment-scorerne mere varieret op til et valg? Bliver de mere varieret omkring 2015?\n",
    "\n",
    "M: Brug sentiment analyse til at identificere forandringer i sentimenter i taler knyttet til emneordet Immigration.\n",
    "Definition: \"SA is a part of applied computational linguistics and attempts to quantify the emotions\" (Kran, E., & Orm, S. (2020)).\n",
    "\n",
    "\n",
    "### Hvilken sentiment analyse tilgang anvender vi?\n",
    "\n",
    "På Alexandra Instituttets DaNLP repository, et repository for Natural Language Processing resources for the Danish Language, leverer Alexandra Instituttet et overblik over open sentiment analysis models and dataset for Danish.\n",
    "\n",
    "Der er følgende modeller:\n",
    "1. AFINN - wordlist model, der returnerer en tal-score, der modsvarer en følelse i et ord. Negativ ( minus ), neutral (0) eller positiv ( plus ) \n",
    "2. Sentida - wordlist model, der ligesom ovenfor returnerer en tal-score, der modsvarer en følelse.\n",
    "3. BERT Emotion - BERT model, der returner en beskrivende tekststreng, der modsvarer et semantisk felt, som et ord er indlejret i, f.eks. glæde/sindsro, forventning/interesse, tillid/accept.     \n",
    "4. BERT Tone - Bert model, der ligeledes returner en beskrivende tekststreng.\n",
    "5. SpaCy Sentiment - Spacy model, der også returner en tekststreng.\n",
    "6. Senda - Bert model, der også returner en tekststreng. \n",
    "\n",
    "De to første, AFINN og Sentida, samt den sidste Senda, er ikke en del af danlp projektet og dermed ikke en del af DaNLP Python projektet, som kan anvendes via pip. \n",
    "\n",
    "Kilde: Alexandra Institute. (2021). Sentiment_analysis.md. https://github.com/alexandrainst/danlp/blob/master/docs/docs/tasks/sentiment_analysis.md\n",
    "\n",
    "\n",
    "I denne opgave vil vi anvende sentiment analyse til at se på variationer i scorerne. Vi kan derfor vælge en af de første to, hvorved vi også vælger ikke at anvende DaNLPs Python pakke. Af de to første wordlist modeller, AFINN og Sentida vælger vi Sentida, fordi modellen er blevet opdateret med ny funktionalitet, og fordi set ud fra Finn Årup Nielsens publicationer siden 2019 har han ikke udgivet nyt omkring sentiment analyse i den periode. \n",
    "\n",
    "Både AFINN og Sentida er wordlist modeller, der aggregerer en sentiment scores baseret på forekomsten af ord fra ordlisten i en given tekst. Den aggregerede score bliver anvendt som en indikator på, hvor positiv teksten \n",
    "er. Der er fire problemer forbundet med denne model:\n",
    "1. modellen tager ikke højde for syntaktiske relation mellem ord.\n",
    "2. modellen ignorerer adverbier, og dermed den betydning adverbierne har i at udtykke grader af noget og holdinger til noget.\n",
    "3. modellen afspejler ikke menneskers måde at opfatte følelser.\n",
    "4. modellen kan ikke håndtere ord der betyder to forskellige ting.\n",
    "Udviklerne bag Sentida har fundet inspiration i den engelsk sprogede VADER model og har forsøgt at minimere problemerne ved at indbygge forskellige, simple former for \"awareness\". For eksempel at forøge score omkring negationer, captital letter og udråbstegn for at give disse elementer i sproget større opmærksomhed. \n",
    "\n",
    "For example:\n",
    "\n",
    "_“Maden (+0.3) var god (+2.3), (← x 0.5) men(1.5 x →) serviceringen (+0.3) var elendig (-4.3).” ⇒ 1.3 -6 ⇒ sentiment score: -4.7 \n",
    "\n",
    "“The  food (+0.3)was  good (+2.3),  (←  x  0.5) but(1.5x  →) the  service (+0.3)was horrendous (-4.3).” ⇒ 1.3 -6 ⇒ sentiment score: -4.7_\n",
    "\n",
    "\n",
    "_“Det er så sejt (+3.6)!(← x 1.291)” ⇒ sentiment score: 4.6\n",
    "\n",
    "“It is so cool (+3.6)!(←x 1.291)”⇒ sentiment score: 4.6_\n",
    "\n",
    "\n",
    "_“DET ERSÅ SEJT (+3.6). (← x 1.733)” ⇒ sentiment score: 6.2\n",
    "\n",
    "“IT IS SO COOL (+3.6). (← x 1.733)” ⇒ sentiment score: 6.2_\n",
    "\n",
    "\n",
    "(Kran, E., & Orm, S. (2020)).\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Neural networks baserede SA modeller som \"aspect-based sentiment analysis\" kan tage højde for ord og kontekst og er på den måde mere præcise, men eftersom neurale networds modeller forudsætter store mængder af træningsdata, er de langsomme og upraktiske at benytte (Kran, E., & Orm, S. (2020))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3878325a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install sentida"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63704f54",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentida import Sentida\n",
    "# Define the class:\n",
    "SV = Sentida()\n",
    "\n",
    "SV.sentida(\n",
    "        text = 'Lad der blive fred.',\n",
    "        output = 'mean',\n",
    "        normal = False,\n",
    "        speed = 'normal')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52a97d2f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b11397dc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c3cd481",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46610c7d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dca251e3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abdb8f4d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a195277",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c0871128",
   "metadata": {},
   "source": [
    "https://github.com/alexandrainst/danlp/blob/master/docs/docs/tasks/sentiment_analysis.md\n",
    "\n",
    "https://github.com/alexandrainst/danlp/blob/master/docs/docs/frameworks/spacy.md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7347ce7d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3de80476",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "240a4ed8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92114b41",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6ba62045",
   "metadata": {},
   "source": [
    "# Litteraturliste\n",
    "\n",
    "Lauridsen, G. A., Dalsgaard, J. A., & Svendsen, L. K. B. (2019). SENTIDA: A New Tool for Sentiment Analysis in Danish. Journal of Language Works - Sprogvidenskabeligt Studentertidsskrift, 4(1), 38–53. Retrieved from https://tidsskrift.dk/lwo/article/view/115711\n",
    "\n",
    "Kran, E., & Orm, S. (2020). EMMA: Danish Natural-Language Processing of Emotion in Text: The new State-of-the-Art in Danish Sentiment Analysis and a Multidimensional Emotional Sentiment Validation Dataset. Journal of Language Works - Sprogvidenskabeligt Studentertidsskrift, 5(1), 92–110. Retrieved from https://tidsskrift.dk/lwo/article/view/121221\n",
    "\n",
    "Brogaard Pauli, Amalie and Barrett, Maria and Lacroix, Ophélie and Hvingelby, Rasmus. (2021) An open-source toolkit for Danish Natural Language Processing. https://ep.liu.se/ecp/178/053/ecp2021178053.pdf \n",
    "\n",
    "Alexandra Institute. (2021). Sentiment_analysis.md. https://github.com/alexandrainst/danlp/blob/master/docs/docs/tasks/sentiment_analysis.md\n",
    "\n",
    "Hansen, Dorte Haltrup and Navarretta, Costanza, 2021, The Danish Parliament Corpus 2009 - 2017, v2, w. subject annotation, CLARIN-DK-UCPH Centre Repository, http://hdl.handle.net/20.500.12115/44.\n",
    "\n",
    "McKinney, Wes. Python for Data Analysis : Data Wrangling with Pandas, NumPy and Jupyter. Third edition. Sebastopol, CA: O’Reilly Media, Inc., 2022"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c2e6e64",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61506c2f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20c61943",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "515f192b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# bump_chart\n",
    "# https://altair-viz.github.io/gallery/bump_chart.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79483881",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
